{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c3b5d9-3761-4913-9840-66048d3fe9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cfea9f-cf92-4c01-9315-72c8499912c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and define the credentials\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "SOURCE_DB_HOST=os.getenv(\"SOURCE_DB_HOST\")\n",
    "SOURCE_DB_USER=os.getenv(\"SOURCE_DB_USER\")\n",
    "SOURCE_DB_PASS=os.getenv(\"SOURCE_DB_PASS\")\n",
    "SOURCE_DB_NAME=os.getenv(\"SOURCE_DB_NAME\")\n",
    "SOURCE_DB_PORT=os.getenv(\"SOURCE_DB_PORT\")\n",
    "\n",
    "STG_DB_HOST=os.getenv(\"STG_DB_HOST\")\n",
    "STG_DB_USER=os.getenv(\"STG_DB_USER\")\n",
    "STG_DB_PASS=os.getenv(\"STG_DB_PASS\")\n",
    "STG_DB_NAME=os.getenv(\"STG_DB_NAME\")\n",
    "STG_DB_PORT=os.getenv(\"STG_DB_PORT\")\n",
    "\n",
    "DWH_DB_HOST=os.getenv(\"DWH_DB_HOST\")\n",
    "DWH_DB_USER=os.getenv(\"DWH_DB_USER\")\n",
    "DWH_DB_PASS=os.getenv(\"DWH_DB_PASS\")\n",
    "DWH_DB_NAME=os.getenv(\"DWH_DB_NAME\")\n",
    "DWH_DB_PORT=os.getenv(\"DWH_DB_PORT\")\n",
    "\n",
    "LOG_DB_HOST=os.getenv(\"LOG_DB_HOST\")\n",
    "LOG_DB_USER=os.getenv(\"LOG_DB_USER\")\n",
    "LOG_DB_PASS=os.getenv(\"LOG_DB_PASS\")\n",
    "LOG_DB_NAME=os.getenv(\"LOG_DB_NAME\")\n",
    "LOG_DB_PORT=os.getenv(\"LOG_DB_PORT\")\n",
    "\n",
    "MINIO_ENDPOINT=os.getenv(\"MINIO_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY=os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY=os.getenv(\"MINIO_SECRET_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c665afae-29b1-448c-acac-8c1ac44c9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Project Dev</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff9fb627f50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PySpark Project Dev\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1ac23-60b3-49d1-9b11-17bfa69f24ab",
   "metadata": {},
   "source": [
    "## Create database engine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d049515c-3812-45c3-ad33-3825bb8652b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_engine():\n",
    "    SOURCE_DB_URL = f\"jdbc:postgresql://{SOURCE_DB_HOST}:{SOURCE_DB_PORT}/{SOURCE_DB_NAME}\"\n",
    "    return SOURCE_DB_URL, SOURCE_DB_USER, SOURCE_DB_PASS \n",
    "\n",
    "def staging_engine():\n",
    "    STG_DB_URL = f\"jdbc:postgresql://{STG_DB_HOST}:{STG_DB_PORT}/{STG_DB_NAME}\"\n",
    "    return STG_DB_URL, STG_DB_USER, STG_DB_PASS \n",
    "    \n",
    "def staging_engine_sqlalchemy():\n",
    "    return create_engine(f\"postgresql://{STG_DB_USER}:{STG_DB_PASS}@{STG_DB_HOST}:{STG_DB_PORT}/{STG_DB_NAME}\")\n",
    "\n",
    "def dwh_engine_sqlalchemy():\n",
    "    return create_engine(f\"postgresql://{DWH_DB_USER}:{DWH_DB_PASS}@{DWH_DB_HOST}:{DWH_DB_PORT}/{DWH_DB_NAME}\")\n",
    "\n",
    "def dwh_engine():\n",
    "    DWH_DB_URL = f\"jdbc:postgresql://{DWH_DB_HOST}:{DWH_DB_PORT}/{DWH_DB_NAME}\"\n",
    "    return DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS \n",
    "\n",
    "def log_engine():\n",
    "    LOG_DB_URL = f\"jdbc:postgresql://{LOG_DB_HOST}:{LOG_DB_PORT}/{LOG_DB_NAME}\"\n",
    "    return LOG_DB_URL, LOG_DB_USER, LOG_DB_PASS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bbf18-6c3a-41cf-ad0b-831cf6730ac1",
   "metadata": {},
   "source": [
    "## Set Up Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca584d3a-a66d-4d09-90ba-1c832134d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_msg(spark: SparkSession, log_msg):\n",
    "\n",
    "    LOG_DB_URL, LOG_DB_USER, LOG_DB_PASS = log_engine()\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": LOG_DB_USER,\n",
    "        \"password\": LOG_DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = LOG_DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1ce7c-be68-4a5d-882e-3a170dc0fdbd",
   "metadata": {},
   "source": [
    "## Extract Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aebcd6fc-3c97-422a-b9c9-6e552c8747d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_database(spark: SparkSession, table_name: str):\n",
    "    \n",
    "    # Get source db config\n",
    "    SOURCE_DB_URL, SOURCE_DB_USER, SOURCE_DB_PASS = source_engine()\n",
    "\n",
    "    # Set config\n",
    "    connection_properties = {\n",
    "        \"user\" : SOURCE_DB_USER,\n",
    "        \"password\" : SOURCE_DB_PASS,\n",
    "        \"driver\" : \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Read data\n",
    "        df = spark \\\n",
    "            .read \\\n",
    "            .jdbc(url=SOURCE_DB_URL,\n",
    "                  table=table_name,\n",
    "                  properties=connection_properties)\n",
    "\n",
    "        print(f\"Extraction process successful for table: {table_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"source_db\", table_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"source_db\", table_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44a63aa-adac-44b7-8286-b46fe74f4ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for table: acquisition\n",
      "Extraction process successful for table: company\n",
      "Extraction process successful for table: funding_rounds\n",
      "Extraction process successful for table: funds\n",
      "Extraction process successful for table: investments\n",
      "Extraction process successful for table: ipos\n"
     ]
    }
   ],
   "source": [
    "acquisition_df = extract_database(spark=spark, table_name=\"acquisition\")\n",
    "company_df = extract_database(spark=spark, table_name=\"company\")\n",
    "funding_rounds_df = extract_database(spark=spark, table_name=\"funding_rounds\")\n",
    "funds_df = extract_database(spark=spark, table_name=\"funds\")\n",
    "investments_df = extract_database(spark=spark, table_name=\"investments\")\n",
    "ipos_df = extract_database(spark=spark, table_name=\"ipos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2dfb8-f000-48bc-b115-30abca4f089a",
   "metadata": {},
   "source": [
    "## Extract CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904c0fa4-dc61-4e37-82d4-00f057022612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(spark: SparkSession, file_name: str):\n",
    "\n",
    "    # Set csv file path\n",
    "    path = \"data/\"\n",
    "    \n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Read data\n",
    "        df = spark.read.csv(path + file_name, header=True)\n",
    "\n",
    "        print(f\"Extraction process successful for file: {file_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"csv\", file_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"csv\", file_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14b40ce-81af-4550-a2c4-be5e55ca0b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for file: people.csv\n",
      "Extraction process successful for file: relationships.csv\n"
     ]
    }
   ],
   "source": [
    "people_df = extract_csv(spark=spark, file_name=\"people.csv\")\n",
    "relationship_df = extract_csv(spark=spark, file_name=\"relationships.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4fa7f-d187-4568-8e62-905f682d8900",
   "metadata": {},
   "source": [
    "## Extract API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f421fd-f4f1-4aab-b1cf-bc722ce0cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_api(spark: SparkSession, start_date: str, end_date:str):\n",
    "\n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    # Define API url\n",
    "    url = f\"https://api-milestones.vercel.app/api/data?start_date={start_date}&end_date={end_date}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            print(\"There is no data in this range of date\")\n",
    "            df = None\n",
    "        else:\n",
    "            print(f\"Extraction process successful for milestones table\")\n",
    "            df = spark.createDataFrame(data)\n",
    "        \n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"api\", \"milestones\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "        df = None\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"api\", \"milestones\", current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f335a2dc-d09d-4d7d-9688-2985dd66f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for milestones table\n"
     ]
    }
   ],
   "source": [
    "milestones_df = extract_api(spark=spark, start_date=\"2014-01-01\", end_date=\"2015-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03aede7-1c86-4be5-a64e-458a69e06078",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99130fbc-4893-493e-8d02-87d0c6b9655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Percentage of Missing Values for each column with pyspark\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql.functions import col, count, when, round\n",
    "\n",
    "def check_missing_values(df):\n",
    "\n",
    "    total_data = df.count()\n",
    "\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    get_missing_values = df.select([\n",
    "        round((count(when(col(column_name).isNull(), column_name)) / total_data) * 100, 2).alias(column_name)\n",
    "        for column_name in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "    \n",
    "    return get_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20676c3-46f4-453e-aeaf-07e7d6f39679",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profiling_report = {\n",
    "    \"Created by\" : \"Rico Febrian\",\n",
    "    \"Checking Date\" : datetime.now().strftime('%d/%m/%y'),\n",
    "    \"Column Information\": {\n",
    "        \"Acquisition\": {\"count\": len(acquisition_df.columns), \"columns\": acquisition_df.columns},\n",
    "        \"Company\": {\"count\": len(company_df.columns), \"columns\": company_df.columns},\n",
    "        \"Funding Rounds\": {\"count\": len(funding_rounds_df.columns), \"columns\": funding_rounds_df.columns},\n",
    "        \"Funds\": {\"count\": len(funds_df.columns), \"columns\": funds_df.columns},\n",
    "        \"Investments\": {\"count\": len(investments_df.columns), \"columns\": investments_df.columns},\n",
    "        \"IPOS\": {\"count\": len(ipos_df.columns), \"columns\": ipos_df.columns},\n",
    "        \"People\": {\"count\": len(people_df.columns), \"columns\": people_df.columns},\n",
    "        \"Relationships\": {\"count\": len(relationship_df.columns), \"columns\": relationship_df.columns},\n",
    "        \"Milestones\": {\"count\": len(milestones_df.columns), \"columns\": milestones_df.columns}\n",
    "    },\n",
    "    \"Check Data Size\": {\n",
    "        \"Acquisition\": acquisition_df.count(),\n",
    "        \"Company\": company_df.count(),\n",
    "        \"Funding Rounds\": funding_rounds_df.count(),\n",
    "        \"Funds\": funds_df.count(),\n",
    "        \"Investments\": investments_df.count(),\n",
    "        \"IPOS\": ipos_df.count(),\n",
    "        \"People\": people_df.count(),\n",
    "        \"Relationships\": relationship_df.count(),\n",
    "        \"Milestones\": milestones_df.count()\n",
    "    },\n",
    "    \"Data Type For Each Column\" : {\n",
    "        \"Acquisition\": acquisition_df.dtypes,\n",
    "        \"Company\": company_df.dtypes,\n",
    "        \"Funding Rounds\": funding_rounds_df.dtypes,\n",
    "        \"Funds\": funds_df.dtypes,\n",
    "        \"Investments\": investments_df.dtypes,\n",
    "        \"IPOS\": ipos_df.dtypes,\n",
    "        \"People\": people_df.dtypes,\n",
    "        \"Relationships\": relationship_df.dtypes,\n",
    "        \"Milestones\": milestones_df.dtypes\n",
    "    },\n",
    "    \"Check Missing Value\" : {\n",
    "        \"Acquisition\": check_missing_values(acquisition_df),\n",
    "        \"Company\": check_missing_values(company_df),\n",
    "        \"Funding Rounds\": check_missing_values(funding_rounds_df),\n",
    "        \"Funds\": check_missing_values(funds_df),\n",
    "        \"Investments\": check_missing_values(investments_df),\n",
    "        \"IPOS\": check_missing_values(ipos_df),\n",
    "        \"People\": check_missing_values(people_df),\n",
    "        \"Relationships\": check_missing_values(relationship_df),\n",
    "        \"Milestones\": check_missing_values(milestones_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print dalam format JSON yang rapi\n",
    "# print(json.dumps(data_profiling_report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30af6fd-14db-4ed4-82d6-b20d8171dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to save the final report to a JSON file\n",
    "def save_to_json(dict_result: dict, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    This function saves the data profiling result to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        dict_result (dict): Data profiling result to save to a JSON file.\n",
    "        filename (str): Name of the JSON file to save the data profiling result to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Save the data profiling result to a JSON file\n",
    "        with open(f'{filename}.json', 'w') as file:\n",
    "            file.write(json.dumps(dict_result, indent= 4))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9e1710-0b85-4d60-9523-f2906837489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(dict_result=data_profiling_report, filename=\"data_profiling_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fe416-633b-4cd9-a7d0-8e2507c081aa",
   "metadata": {},
   "source": [
    "## Dump to Data Lake (MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87364440-d5cc-4ec5-9e00-ea1e38014bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_to_minio(df, minio_path):\n",
    "\n",
    "#     try:\n",
    "#         df.write \\\n",
    "#           .mode(\"overwrite\") \\\n",
    "#           .parquet(f\"s3a://{minio_path}\")\n",
    "\n",
    "#         print(f\"Initial load completed to {minio_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to load data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253f4c90-5579-4f7c-a2eb-5e602f8cc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_to_minio(df=acquisition_df, minio_path=\"raw-data/acquisition_table\")\n",
    "# load_to_minio(df=company_df, minio_path=\"raw-data/company_table\")\n",
    "# load_to_minio(df=funding_rounds_df, minio_path=\"raw-data/funding_rounds_table\")\n",
    "# load_to_minio(df=funds_df, minio_path=\"raw-data/funds_table\")\n",
    "# load_to_minio(df=investments_df, minio_path=\"raw-data/investments_table\")\n",
    "# load_to_minio(df=ipos_df, minio_path=\"raw-data/ipos_table\")\n",
    "# load_to_minio(df=people_df, minio_path=\"raw-data/people_table\")\n",
    "# load_to_minio(df=relationship_df, minio_path=\"raw-data/relationships_table\")\n",
    "# load_to_minio(df=milestones_df, minio_path=\"raw-data/milestones_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e51874-f4da-45cc-9d99-7aa5db98cbec",
   "metadata": {},
   "source": [
    "## Load to Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d11b4cb-0335-4dfa-8650-f982ea14b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_stg(spark, df, table_name, source_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Establish connection to staging db\n",
    "        conn = staging_engine_sqlalchemy()\n",
    "\n",
    "        with conn.begin() as connection:\n",
    "\n",
    "            # Truncate all tables in data warehouse\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "        print(f\"Success truncating table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error when truncating table: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "\n",
    "    finally:\n",
    "        conn.dispose()\n",
    "\n",
    "    # Load extarcted DataFrame to staging db\n",
    "    try:\n",
    "        \n",
    "        # Get staging db config\n",
    "        STG_DB_URL, STG_DB_USER, STG_DB_PASS = staging_engine()\n",
    "    \n",
    "        # Set config\n",
    "        properties = {\n",
    "            \"user\" : STG_DB_USER,\n",
    "            \"password\" : STG_DB_PASS,\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=STG_DB_URL,\n",
    "                      table=table_name,\n",
    "                      mode=\"append\",\n",
    "                      properties=properties)\n",
    "\n",
    "        print(f\"Load process successful for table: {table_name}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Load process failed: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2829a218-c4dd-4c77-a504-5af2483b8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: people\n",
      "Load process successful for table: people\n",
      "Success truncating table: relationships\n",
      "Load process successful for table: relationships\n",
      "Success truncating table: company\n",
      "Load process successful for table: company\n",
      "Success truncating table: funding_rounds\n",
      "Load process successful for table: funding_rounds\n",
      "Success truncating table: funds\n",
      "Load process successful for table: funds\n",
      "Success truncating table: acquisition\n",
      "Load process successful for table: acquisition\n",
      "Success truncating table: ipos\n",
      "Load process successful for table: ipos\n",
      "Success truncating table: investments\n",
      "Load process successful for table: investments\n",
      "Success truncating table: milestones\n",
      "Load process successful for table: milestones\n"
     ]
    }
   ],
   "source": [
    "load_to_stg(spark=spark, df=people_df, table_name=\"people\", source_name=\"csv\")\n",
    "load_to_stg(spark=spark, df=relationship_df, table_name=\"relationships\", source_name=\"csv\")\n",
    "load_to_stg(spark=spark, df=company_df, table_name=\"company\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=funding_rounds_df, table_name=\"funding_rounds\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=funds_df, table_name=\"funds\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=acquisition_df, table_name=\"acquisition\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=ipos_df, table_name=\"ipos\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=investments_df, table_name=\"investments\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=milestones_df, table_name=\"milestones\", source_name=\"api\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca71476f-3bf9-491b-ada6-b9bf59c1240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2352d46-417b-41ad-8104-9c18ab56174f",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd128ef7-f1f7-41af-b75f-8d46c4816c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform_company(spark, df):\n",
    "    \n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Add new column, entity type\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"entity_type\",\n",
    "            F.when(F.col(\"object_id\").startswith(\"c:\"), \"company\")\n",
    "             .when(F.col(\"object_id\").startswith(\"f:\"), \"fund\")\n",
    "             .otherwise(None)\n",
    "        )\n",
    "\n",
    "        # Concat address\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"full_address\",\n",
    "            F.concat_ws(\", \", F.col(\"address1\"), F.col(\"address2\"))\n",
    "        )\n",
    "\n",
    "        # Select columns\n",
    "        dim_company = df.select(\n",
    "            F.col(\"object_id\").alias(\"nk_company_id\"),\n",
    "            F.col(\"entity_type\"),\n",
    "            F.col(\"full_address\"),\n",
    "            F.col(\"region\"),\n",
    "            F.col(\"city\"),\n",
    "            F.col(\"country_code\")\n",
    "        )\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "        .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_company\", current_timestamp)])\\\n",
    "        .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return dim_company\n",
    "        \n",
    "    except Exception as e:\n",
    "            #log message\n",
    "            print(e)\n",
    "            log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_company\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09ee0284-a089-4834-bf27-36d2e656c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_company = transform_company(spark, df=company_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "09606bd3-f381-4eb6-b6f7-7289b64451c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_people(spark, df):\n",
    "    \n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Concat first and last name\n",
    "\n",
    "        df = df.withColumn(\n",
    "            \"full_name\",\n",
    "            F.concat_ws(\" \", F.col(\"first_name\"), F.col(\"last_name\"))\n",
    "        )\n",
    "\n",
    "        # Select columns\n",
    "        dim_people = df.select(\n",
    "            F.col(\"object_id\").alias(\"nk_people_id\"),\n",
    "            F.col(\"full_name\"),\n",
    "            F.col(\"affiliation_name\")\n",
    "        )\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "        .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_people\", current_timestamp)])\\\n",
    "        .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return dim_people\n",
    "        \n",
    "    except Exception as e:\n",
    "            #log message\n",
    "            print(e)\n",
    "            log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_people\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0b9fb5b7-cf67-40be-97d7-3c73974b719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_people = transform_people(spark, df=people_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bc4bd257-aca1-42e9-a576-415bbcdb0986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+\n",
      "|nk_people_id|       full_name|    affiliation_name|\n",
      "+------------+----------------+--------------------+\n",
      "|         p:2|     Ben Elowitz|           Blue Nile|\n",
      "|         p:3|  Kevin Flaherty|            Wetpaint|\n",
      "|         p:4|    Raju Vegesna|                Zoho|\n",
      "|         p:5|       Ian Wenig|                Zoho|\n",
      "|         p:6|      Kevin Rose|        i/o Ventures|\n",
      "|         p:7|     Jay Adelson|                Digg|\n",
      "|         p:8|      Owen Byrne|                Digg|\n",
      "|         p:9|  Ron Gorodetzky|                Digg|\n",
      "|        p:10| Mark Zuckerberg|            Facebook|\n",
      "|        p:11|Dustin Moskovitz|            Facebook|\n",
      "|        p:12|  Owen Van Natta|               Asana|\n",
      "|        p:13|     Matt Cohler|            LinkedIn|\n",
      "|        p:14|    Chris Hughes|General Catalyst ...|\n",
      "|        p:16|      Alex Welch|            C7 Group|\n",
      "|        p:17|  Darren Crystal|         Photobucket|\n",
      "|        p:18|   Michael Clark|   Photobucket (Old)|\n",
      "|        p:19|     Greg Wimmer|         Photobucket|\n",
      "|        p:20|    Peter Foster|         Photobucket|\n",
      "|        p:21|    Heather Dana|         Photobucket|\n",
      "|        p:22|      Peter Pham|   Photobucket, Inc.|\n",
      "+------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12cca8cc-5462-4622-94c0-867ad8d40b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_warehouse(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS = dwh_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DWH_DB_USER,\n",
    "        \"password\": DWH_DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DWH_DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b394647a-019c-471f-a770-64202466e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fund(spark, df):\n",
    "    \n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Extract dim_date\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "\n",
    "        # Rename columns\n",
    "        df = df.withColumnRenamed(\"name\", \"fund_name\") \\\n",
    "               .withColumnRenamed(\"source_description\", \"fund_description\")\n",
    "\n",
    "        # Add new column and convert all raised amount to USD using when/otherwise\n",
    "        df = df.withColumn(\n",
    "            \"raised_amount_usd\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"raised_currency_code\") == \"USD\", F.col(\"raised_amount\"))\n",
    "                 .when(F.col(\"raised_currency_code\") == \"CAD\", F.col(\"raised_amount\") * 0.72)\n",
    "                 .when(F.col(\"raised_currency_code\") == \"EUR\", F.col(\"raised_amount\") * 1.14)\n",
    "                 .when(F.col(\"raised_currency_code\") == \"SEK\", F.col(\"raised_amount\") * 0.10)\n",
    "                 .when(F.col(\"raised_currency_code\") == \"AUD\", F.col(\"raised_amount\") * 0.64)\n",
    "                 .when(F.col(\"raised_currency_code\") == \"JPY\", F.col(\"raised_amount\") * 0.007)\n",
    "                 .when(F.col(\"raised_currency_code\") == \"GBP\", F.col(\"raised_amount\") * 1.33)\n",
    "                 .otherwise(F.col(\"raised_amount\")),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add Foreign Key to dim_date\n",
    "\n",
    "        # Generate new column based on funded_at with same format with date_id in dim_date table\n",
    "        df = df.withColumn(\n",
    "            \"funded_date_id\",\n",
    "            F.date_format(df.funded_at, \"yyyyMMdd\").cast(\"integer\")\n",
    "        )\n",
    "\n",
    "        # Get date_id from dim_date based on funded_date_id column\n",
    "        df = df.join(\n",
    "            dim_date,\n",
    "            df.funded_date_id == dim_date.date_id,\n",
    "            \"left\"\n",
    "        )\n",
    "\n",
    "        # Remove empty string in fund_description column and mark as null\n",
    "        df = df.withColumn(\n",
    "            \"fund_description\",\n",
    "            F.when(F.trim(df.fund_description) == \"\", None)\n",
    "             .otherwise(df.fund_description)\n",
    "        )\n",
    "        \n",
    "        # Select columns\n",
    "        dim_fund = df.select(\n",
    "            F.col(\"object_id\").alias(\"nk_fund_id\"),\n",
    "            F.col(\"fund_name\"),\n",
    "            F.col(\"raised_amount_usd\"),\n",
    "            F.col(\"funded_date_id\").alias(\"funded_at\"),\n",
    "            F.col(\"fund_description\")\n",
    "        )\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "        .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_fund\", current_timestamp)])\\\n",
    "        .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return dim_fund\n",
    "        \n",
    "    except Exception as e:\n",
    "            #log message\n",
    "            print(e)\n",
    "            log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_fund\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e634df1f-f970-4716-8650-180dba9e253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_fund = transform_fund(spark, df=funds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "63873fab-8eae-40be-ab8f-97a345ed09f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: dim_fund\n",
      "Load process successful for table: dim_fund\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=dim_fund, table_name=\"dim_fund\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f726a744-baf7-4407-9007-3baa741b2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_relationship(spark, df):\n",
    "    \n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # sk_company_id,\n",
    "        # sk_people_id,\n",
    "        # title,\n",
    "        # is_past,\n",
    "        # relationship_start_at,\n",
    "        # relationship_end_at\n",
    "        \n",
    "        # Extract selected tables\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "        dim_company = extract_warehouse(spark, table_name=\"dim_company\")\n",
    "        dim_people = extract_warehouse(spark, table_name=\"dim_people\")\n",
    "\n",
    "        # Add Foreign Key to dim_company\n",
    "\n",
    "        # Create sk_company_id column based on dim_company\n",
    "        df = df.join(\n",
    "            dim_company.select(\"sk_company_id\", \"nk_company_id\"),\n",
    "            df.relationship_object_id == dim_company.nk_company_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Create sk_people_id column based on dim_people\n",
    "        df = df.join(\n",
    "            dim_people.select(\"sk_people_id\", \"nk_people_id\"),\n",
    "            df.person_object_id == dim_people.nk_people_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # .....\n",
    "        df = df.withColumn(\"relationship_start_at\", F.date_format(\"start_at\", \"yyyyMMdd\").cast(\"integer\")) \\\n",
    "               .withColumn(\"relationship_end_at\", F.date_format(\"end_at\", \"yyyyMMdd\").cast(\"integer\"))\n",
    "\n",
    "        dim_date_start = dim_date.alias(\"start_date\")\n",
    "        dim_date_end = dim_date.alias(\"end_date\")\n",
    "        \n",
    "        df = df.join(\n",
    "            dim_date_start,\n",
    "            df.relationship_start_at == F.col(\"start_date.date_id\"),\n",
    "            \"inner\"\n",
    "        ).join(\n",
    "            dim_date_end,\n",
    "            df.relationship_end_at == F.col(\"end_date.date_id\"),\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Select columns\n",
    "        bridge_company_people = df.select(\n",
    "            F.col(\"sk_company_id\"),\n",
    "            F.col(\"sk_people_id\"),\n",
    "            F.col(\"title\"),\n",
    "            F.col(\"is_past\"),\n",
    "            F.col(\"relationship_start_at\"),\n",
    "            F.col(\"relationship_end_at\")\n",
    "        )\n",
    "        \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "        .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"bridge_company_people\", current_timestamp)])\\\n",
    "        .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return bridge_company_people\n",
    "        \n",
    "    except Exception as e:\n",
    "            #log message\n",
    "            print(e)\n",
    "            log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"bridge_company_people\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "09cf0d40-e4bf-4ba3-a647-44ba64e98734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------------+-------+---------------------+-------------------+\n",
      "|sk_company_id|sk_people_id|               title|is_past|relationship_start_at|relationship_end_at|\n",
      "+-------------+------------+--------------------+-------+---------------------+-------------------+\n",
      "|            1|      180703|Sr. EMEA Operatio...|  false|             20050101|           20080101|\n",
      "|            1|       53575|        Board Member|  false|             20010101|           20070101|\n",
      "|            1|         599|Executive Vice Pr...|  false|             20000801|           20070901|\n",
      "|            2|      168571|          Co-founder|  false|             20060101|           20100101|\n",
      "|            2|      101437|Designer & Co-fou...|  false|             20060701|           20070201|\n",
      "|            2|      101437|          VP Product|  false|             20070201|           20090601|\n",
      "|            2|      101437|                 COO|  false|             20090601|           20100701|\n",
      "|            2|         440|  Founder & Chairman|  false|             20060601|           20090701|\n",
      "|            3|      106882|       VP of Product|  false|             20060601|           20090101|\n",
      "|            3|      106158|    VP of Technology|  false|             20060601|           20090101|\n",
      "|            3|         457|     COO, Co-Founder|  false|             20060515|           20080701|\n",
      "|            3|         455|  Co-founder and CEO|  false|             20060501|           20080301|\n",
      "|            4|      198149|    SVP & GM, Mobile|  false|             20010801|           20090201|\n",
      "|            4|         394|        V.P. Finance|  false|             20070501|           20081001|\n",
      "|            4|      205857|Vice President, E...|  false|             20070701|           20080601|\n",
      "|            4|      199523|Director of Audie...|  false|             20090101|           20110201|\n",
      "|            4|      172783|  Technology Manager|  false|             20060801|           20061223|\n",
      "|            4|      159666|            Director|  false|             20070101|           20080101|\n",
      "|            4|       11867|           President|  false|             20050301|           20071201|\n",
      "|            4|      142002|      Vice President|  false|             20040601|           20091001|\n",
      "+-------------+------------+--------------------+-------+---------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bridge_company_people = transform_relationship(spark, df=relationship_df)\n",
    "\n",
    "bridge_company_people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eca4abab-154b-4b18-91e1-ffa93d227f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: bridge_company_people\n",
      "Load process successful for table: bridge_company_people\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=bridge_company_people, table_name=\"bridge_company_people\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca5639-1907-40d0-ab6c-99991046263a",
   "metadata": {},
   "source": [
    "## Load to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaaffeb0-a6f3-42c3-9334-5fc5fd56de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_dwh(spark, df, table_name, source_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Establish connection to warehouse db\n",
    "        conn = dwh_engine_sqlalchemy()\n",
    "\n",
    "        with conn.begin() as connection:\n",
    "\n",
    "            # Truncate all tables in data warehouse\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "        print(f\"Success truncating table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error when truncating table: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "\n",
    "    finally:\n",
    "        conn.dispose()\n",
    "\n",
    "    # Load extarcted DataFrame to warehouse db\n",
    "    try:\n",
    "        \n",
    "        # Get warehouse db config\n",
    "        DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS = dwh_engine()\n",
    "    \n",
    "        # Set config\n",
    "        properties = {\n",
    "            \"user\" : DWH_DB_USER,\n",
    "            \"password\" : DWH_DB_PASS,\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=DWH_DB_URL,\n",
    "                      table=table_name,\n",
    "                      mode=\"append\",\n",
    "                      properties=properties)\n",
    "\n",
    "        print(f\"Load process successful for table: {table_name}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Load process failed: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1c6ab97-5d82-4453-b3dc-5c98d8ade0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: dim_company\n",
      "Load process successful for table: dim_company\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=dim_company ,table_name=\"dim_company\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f5752696-ca35-4c95-bbd4-3bbdf45ea452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: dim_people\n",
      "Load process successful for table: dim_people\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=dim_people, table_name=\"dim_people\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c7375-5c50-4e19-9d30-ab82268f520f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
