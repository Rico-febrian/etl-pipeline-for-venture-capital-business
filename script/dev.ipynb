{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c3b5d9-3761-4913-9840-66048d3fe9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cfea9f-cf92-4c01-9315-72c8499912c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env and define the credentials\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "SOURCE_DB_HOST=os.getenv(\"SOURCE_DB_HOST\")\n",
    "SOURCE_DB_USER=os.getenv(\"SOURCE_DB_USER\")\n",
    "SOURCE_DB_PASS=os.getenv(\"SOURCE_DB_PASS\")\n",
    "SOURCE_DB_NAME=os.getenv(\"SOURCE_DB_NAME\")\n",
    "SOURCE_DB_PORT=os.getenv(\"SOURCE_DB_PORT\")\n",
    "\n",
    "STG_DB_HOST=os.getenv(\"STG_DB_HOST\")\n",
    "STG_DB_USER=os.getenv(\"STG_DB_USER\")\n",
    "STG_DB_PASS=os.getenv(\"STG_DB_PASS\")\n",
    "STG_DB_NAME=os.getenv(\"STG_DB_NAME\")\n",
    "STG_DB_PORT=os.getenv(\"STG_DB_PORT\")\n",
    "\n",
    "DWH_DB_HOST=os.getenv(\"DWH_DB_HOST\")\n",
    "DWH_DB_USER=os.getenv(\"DWH_DB_USER\")\n",
    "DWH_DB_PASS=os.getenv(\"DWH_DB_PASS\")\n",
    "DWH_DB_NAME=os.getenv(\"DWH_DB_NAME\")\n",
    "DWH_DB_PORT=os.getenv(\"DWH_DB_PORT\")\n",
    "\n",
    "LOG_DB_HOST=os.getenv(\"LOG_DB_HOST\")\n",
    "LOG_DB_USER=os.getenv(\"LOG_DB_USER\")\n",
    "LOG_DB_PASS=os.getenv(\"LOG_DB_PASS\")\n",
    "LOG_DB_NAME=os.getenv(\"LOG_DB_NAME\")\n",
    "LOG_DB_PORT=os.getenv(\"LOG_DB_PORT\")\n",
    "\n",
    "API_BASE_URL=os.getenv(\"API_BASE_URL\")\n",
    "\n",
    "MINIO_ENDPOINT=os.getenv(\"MINIO_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY=os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY=os.getenv(\"MINIO_SECRET_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c665afae-29b1-448c-acac-8c1ac44c9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Project Dev</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fce8fe105d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PySpark Project Dev\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1ac23-60b3-49d1-9b11-17bfa69f24ab",
   "metadata": {},
   "source": [
    "## Create database engine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d049515c-3812-45c3-ad33-3825bb8652b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_engine():\n",
    "    SOURCE_DB_URL = f\"jdbc:postgresql://{SOURCE_DB_HOST}:{SOURCE_DB_PORT}/{SOURCE_DB_NAME}\"\n",
    "    return SOURCE_DB_URL, SOURCE_DB_USER, SOURCE_DB_PASS \n",
    "\n",
    "def staging_engine():\n",
    "    STG_DB_URL = f\"jdbc:postgresql://{STG_DB_HOST}:{STG_DB_PORT}/{STG_DB_NAME}\"\n",
    "    return STG_DB_URL, STG_DB_USER, STG_DB_PASS \n",
    "    \n",
    "def staging_engine_sqlalchemy():\n",
    "    return create_engine(f\"postgresql://{STG_DB_USER}:{STG_DB_PASS}@{STG_DB_HOST}:{STG_DB_PORT}/{STG_DB_NAME}\")\n",
    "\n",
    "def dwh_engine_sqlalchemy():\n",
    "    return create_engine(f\"postgresql://{DWH_DB_USER}:{DWH_DB_PASS}@{DWH_DB_HOST}:{DWH_DB_PORT}/{DWH_DB_NAME}\")\n",
    "\n",
    "def dwh_engine():\n",
    "    DWH_DB_URL = f\"jdbc:postgresql://{DWH_DB_HOST}:{DWH_DB_PORT}/{DWH_DB_NAME}\"\n",
    "    return DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS \n",
    "\n",
    "def log_engine():\n",
    "    LOG_DB_URL = f\"jdbc:postgresql://{LOG_DB_HOST}:{LOG_DB_PORT}/{LOG_DB_NAME}\"\n",
    "    return LOG_DB_URL, LOG_DB_USER, LOG_DB_PASS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bbf18-6c3a-41cf-ad0b-831cf6730ac1",
   "metadata": {},
   "source": [
    "## Set Up Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca584d3a-a66d-4d09-90ba-1c832134d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_msg(spark: SparkSession, log_msg: pyspark.sql.DataFrame):\n",
    "\n",
    "    LOG_DB_URL, LOG_DB_USER, LOG_DB_PASS = log_engine()\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": LOG_DB_USER,\n",
    "        \"password\": LOG_DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = LOG_DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1ce7c-be68-4a5d-882e-3a170dc0fdbd",
   "metadata": {},
   "source": [
    "## Extract Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aebcd6fc-3c97-422a-b9c9-6e552c8747d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_database(spark: SparkSession, table_name: str):\n",
    "    \n",
    "    # Get source db config\n",
    "    SOURCE_DB_URL, SOURCE_DB_USER, SOURCE_DB_PASS = source_engine()\n",
    "\n",
    "    # Set config\n",
    "    connection_properties = {\n",
    "        \"user\" : SOURCE_DB_USER,\n",
    "        \"password\" : SOURCE_DB_PASS,\n",
    "        \"driver\" : \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Read data\n",
    "        df = spark \\\n",
    "            .read \\\n",
    "            .jdbc(url=SOURCE_DB_URL,\n",
    "                  table=table_name,\n",
    "                  properties=connection_properties)\n",
    "\n",
    "        print(f\"Extraction process successful for table: {table_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"source_db\", table_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"source_db\", table_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44a63aa-adac-44b7-8286-b46fe74f4ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for table: acquisition\n",
      "Extraction process successful for table: company\n",
      "Extraction process successful for table: funding_rounds\n",
      "Extraction process successful for table: funds\n",
      "Extraction process successful for table: investments\n",
      "Extraction process successful for table: ipos\n"
     ]
    }
   ],
   "source": [
    "acquisition_df = extract_database(spark=spark, table_name=\"acquisition\")\n",
    "company_df = extract_database(spark=spark, table_name=\"company\")\n",
    "funding_rounds_df = extract_database(spark=spark, table_name=\"funding_rounds\")\n",
    "funds_df = extract_database(spark=spark, table_name=\"funds\")\n",
    "investments_df = extract_database(spark=spark, table_name=\"investments\")\n",
    "ipos_df = extract_database(spark=spark, table_name=\"ipos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2dfb8-f000-48bc-b115-30abca4f089a",
   "metadata": {},
   "source": [
    "## Extract CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904c0fa4-dc61-4e37-82d4-00f057022612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv(spark: SparkSession, file_name: str):\n",
    "\n",
    "    # Set csv file path\n",
    "    path = \"data/\"\n",
    "    \n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Read data\n",
    "        df = spark.read.csv(path + file_name, header=True)\n",
    "\n",
    "        print(f\"Extraction process successful for file: {file_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"csv\", file_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"csv\", file_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14b40ce-81af-4550-a2c4-be5e55ca0b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for file: people.csv\n",
      "Extraction process successful for file: relationships.csv\n"
     ]
    }
   ],
   "source": [
    "people_df = extract_csv(spark=spark, file_name=\"people.csv\")\n",
    "relationship_df = extract_csv(spark=spark, file_name=\"relationships.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4fa7f-d187-4568-8e62-905f682d8900",
   "metadata": {},
   "source": [
    "## Extract API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f421fd-f4f1-4aab-b1cf-bc722ce0cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_api(spark: SparkSession, start_date: str, end_date:str):\n",
    "\n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    # Define API url\n",
    "    url = f\"{API_BASE_URL}?start_date={start_date}&end_date={end_date}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            print(\"There is no data in this range of date\")\n",
    "            df = None\n",
    "        else:\n",
    "            print(f\"Extraction process successful for milestones table\")\n",
    "            df = spark.createDataFrame(data)\n",
    "        \n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"success\", \"api\", \"milestones\", current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "        df = None\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"sources\", \"extraction\", \"failed\", \"api\", \"milestones\", current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f335a2dc-d09d-4d7d-9688-2985dd66f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for milestones table\n"
     ]
    }
   ],
   "source": [
    "milestones_df = extract_api(spark=spark, start_date=\"2014-01-01\", end_date=\"2015-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03aede7-1c86-4be5-a64e-458a69e06078",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99130fbc-4893-493e-8d02-87d0c6b9655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check Percentage of Missing Values for each column with pyspark\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# from pyspark.sql.functions import col, count, when, round\n",
    "\n",
    "# def check_missing_values(df):\n",
    "\n",
    "#     total_data = df.count()\n",
    "\n",
    "#     # Calculate the percentage of missing values for each column\n",
    "#     get_missing_values = df.select([\n",
    "#         round((count(when(col(column_name).isNull(), column_name)) / total_data) * 100, 2).alias(column_name)\n",
    "#         for column_name in df.columns\n",
    "#     ]).collect()[0].asDict()\n",
    "    \n",
    "#     return get_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20676c3-46f4-453e-aeaf-07e7d6f39679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_profiling_report = {\n",
    "#     \"Created by\" : \"Rico Febrian\",\n",
    "#     \"Checking Date\" : datetime.now().strftime('%d/%m/%y'),\n",
    "#     \"Column Information\": {\n",
    "#         \"Acquisition\": {\"count\": len(acquisition_df.columns), \"columns\": acquisition_df.columns},\n",
    "#         \"Company\": {\"count\": len(company_df.columns), \"columns\": company_df.columns},\n",
    "#         \"Funding Rounds\": {\"count\": len(funding_rounds_df.columns), \"columns\": funding_rounds_df.columns},\n",
    "#         \"Funds\": {\"count\": len(funds_df.columns), \"columns\": funds_df.columns},\n",
    "#         \"Investments\": {\"count\": len(investments_df.columns), \"columns\": investments_df.columns},\n",
    "#         \"IPOS\": {\"count\": len(ipos_df.columns), \"columns\": ipos_df.columns},\n",
    "#         \"People\": {\"count\": len(people_df.columns), \"columns\": people_df.columns},\n",
    "#         \"Relationships\": {\"count\": len(relationship_df.columns), \"columns\": relationship_df.columns},\n",
    "#         \"Milestones\": {\"count\": len(milestones_df.columns), \"columns\": milestones_df.columns}\n",
    "#     },\n",
    "#     \"Check Data Size\": {\n",
    "#         \"Acquisition\": acquisition_df.count(),\n",
    "#         \"Company\": company_df.count(),\n",
    "#         \"Funding Rounds\": funding_rounds_df.count(),\n",
    "#         \"Funds\": funds_df.count(),\n",
    "#         \"Investments\": investments_df.count(),\n",
    "#         \"IPOS\": ipos_df.count(),\n",
    "#         \"People\": people_df.count(),\n",
    "#         \"Relationships\": relationship_df.count(),\n",
    "#         \"Milestones\": milestones_df.count()\n",
    "#     },\n",
    "#     \"Data Type For Each Column\" : {\n",
    "#         \"Acquisition\": acquisition_df.dtypes,\n",
    "#         \"Company\": company_df.dtypes,\n",
    "#         \"Funding Rounds\": funding_rounds_df.dtypes,\n",
    "#         \"Funds\": funds_df.dtypes,\n",
    "#         \"Investments\": investments_df.dtypes,\n",
    "#         \"IPOS\": ipos_df.dtypes,\n",
    "#         \"People\": people_df.dtypes,\n",
    "#         \"Relationships\": relationship_df.dtypes,\n",
    "#         \"Milestones\": milestones_df.dtypes\n",
    "#     },\n",
    "#     \"Check Missing Value\" : {\n",
    "#         \"Acquisition\": check_missing_values(acquisition_df),\n",
    "#         \"Company\": check_missing_values(company_df),\n",
    "#         \"Funding Rounds\": check_missing_values(funding_rounds_df),\n",
    "#         \"Funds\": check_missing_values(funds_df),\n",
    "#         \"Investments\": check_missing_values(investments_df),\n",
    "#         \"IPOS\": check_missing_values(ipos_df),\n",
    "#         \"People\": check_missing_values(people_df),\n",
    "#         \"Relationships\": check_missing_values(relationship_df),\n",
    "#         \"Milestones\": check_missing_values(milestones_df)\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Print dalam format JSON yang rapi\n",
    "# # print(json.dumps(data_profiling_report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30af6fd-14db-4ed4-82d6-b20d8171dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to save the final report to a JSON file\n",
    "# def save_to_json(dict_result: dict, filename: str) -> None:\n",
    "#     \"\"\"\n",
    "#     This function saves the data profiling result to a JSON file.\n",
    "\n",
    "#     Args:\n",
    "#         dict_result (dict): Data profiling result to save to a JSON file.\n",
    "#         filename (str): Name of the JSON file to save the data profiling result to.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "        \n",
    "#         # Save the data profiling result to a JSON file\n",
    "#         with open(f'{filename}.json', 'w') as file:\n",
    "#             file.write(json.dumps(dict_result, indent= 4))\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9e1710-0b85-4d60-9523-f2906837489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_json(dict_result=data_profiling_report, filename=\"data_profiling_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fe416-633b-4cd9-a7d0-8e2507c081aa",
   "metadata": {},
   "source": [
    "## Dump to Data Lake (MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87364440-d5cc-4ec5-9e00-ea1e38014bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_to_minio(df, minio_path):\n",
    "\n",
    "#     try:\n",
    "#         df.write \\\n",
    "#           .mode(\"overwrite\") \\\n",
    "#           .parquet(f\"s3a://{minio_path}\")\n",
    "\n",
    "#         print(f\"Initial load completed to {minio_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to load data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253f4c90-5579-4f7c-a2eb-5e602f8cc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_to_minio(df=acquisition_df, minio_path=\"raw-data/acquisition_table\")\n",
    "# load_to_minio(df=company_df, minio_path=\"raw-data/company_table\")\n",
    "# load_to_minio(df=funding_rounds_df, minio_path=\"raw-data/funding_rounds_table\")\n",
    "# load_to_minio(df=funds_df, minio_path=\"raw-data/funds_table\")\n",
    "# load_to_minio(df=investments_df, minio_path=\"raw-data/investments_table\")\n",
    "# load_to_minio(df=ipos_df, minio_path=\"raw-data/ipos_table\")\n",
    "# load_to_minio(df=people_df, minio_path=\"raw-data/people_table\")\n",
    "# load_to_minio(df=relationship_df, minio_path=\"raw-data/relationships_table\")\n",
    "# load_to_minio(df=milestones_df, minio_path=\"raw-data/milestones_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e51874-f4da-45cc-9d99-7aa5db98cbec",
   "metadata": {},
   "source": [
    "## Load to Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d11b4cb-0335-4dfa-8650-f982ea14b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_stg(spark, df, table_name, source_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Establish connection to staging db\n",
    "        conn = staging_engine_sqlalchemy()\n",
    "\n",
    "        with conn.begin() as connection:\n",
    "\n",
    "            # Truncate all tables in data warehouse\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "        print(f\"Success truncating table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error when truncating table: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "\n",
    "    finally:\n",
    "        conn.dispose()\n",
    "\n",
    "    # Load extarcted DataFrame to staging db\n",
    "    try:\n",
    "        \n",
    "        # Get staging db config\n",
    "        STG_DB_URL, STG_DB_USER, STG_DB_PASS = staging_engine()\n",
    "    \n",
    "        # Set config\n",
    "        properties = {\n",
    "            \"user\" : STG_DB_USER,\n",
    "            \"password\" : STG_DB_PASS,\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=STG_DB_URL,\n",
    "                      table=table_name,\n",
    "                      mode=\"append\",\n",
    "                      properties=properties)\n",
    "\n",
    "        print(f\"Load process successful for table: {table_name}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Load process failed: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2829a218-c4dd-4c77-a504-5af2483b8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: people\n",
      "Load process successful for table: people\n",
      "Success truncating table: relationships\n",
      "Load process successful for table: relationships\n",
      "Success truncating table: company\n",
      "Load process successful for table: company\n",
      "Success truncating table: funding_rounds\n",
      "Load process successful for table: funding_rounds\n",
      "Success truncating table: funds\n",
      "Load process successful for table: funds\n",
      "Success truncating table: acquisition\n",
      "Load process successful for table: acquisition\n",
      "Success truncating table: ipos\n",
      "Load process successful for table: ipos\n",
      "Success truncating table: investments\n",
      "Load process successful for table: investments\n",
      "Success truncating table: milestones\n",
      "Load process successful for table: milestones\n"
     ]
    }
   ],
   "source": [
    "load_to_stg(spark=spark, df=people_df, table_name=\"people\", source_name=\"csv\")\n",
    "load_to_stg(spark=spark, df=relationship_df, table_name=\"relationships\", source_name=\"csv\")\n",
    "load_to_stg(spark=spark, df=company_df, table_name=\"company\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=funding_rounds_df, table_name=\"funding_rounds\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=funds_df, table_name=\"funds\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=acquisition_df, table_name=\"acquisition\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=ipos_df, table_name=\"ipos\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=investments_df, table_name=\"investments\", source_name=\"source_db\")\n",
    "load_to_stg(spark=spark, df=milestones_df, table_name=\"milestones\", source_name=\"api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d68c15-0e09-4091-b04e-f1959bd02804",
   "metadata": {},
   "source": [
    "## Extract data from Staging DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16d1f29-0701-41e2-a510-2e75ab95f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_staging(spark: SparkSession, table_name: str):\n",
    "    \n",
    "    # Get staging db config\n",
    "    STG_DB_URL, STG_DB_USER, STG_DB_PASS = staging_engine()\n",
    "\n",
    "    # Set config\n",
    "    connection_properties = {\n",
    "        \"user\" : STG_DB_USER,\n",
    "        \"password\" : STG_DB_PASS,\n",
    "        \"driver\" : \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Set current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Read data\n",
    "        df = spark \\\n",
    "            .read \\\n",
    "            .jdbc(url=STG_DB_URL,\n",
    "                  table=table_name,\n",
    "                  properties=connection_properties)\n",
    "\n",
    "        print(f\"Extraction process successful for table: {table_name}\")\n",
    "\n",
    "        # Set success log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"source_db\", table_name, current_timestamp)]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\"])\n",
    "    \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction process failed: {e}\")\n",
    "\n",
    "        # Set failed log message\n",
    "        log_message = spark.sparkContext \\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"failed\", \"source_db\", table_name, current_timestamp, str(e))]) \\\n",
    "            .toDF([\"step\", \"process\", \"status\", \"source\", \"table_name\", \"etl_date\", \"error_msg\"])\n",
    "\n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac9db5f8-5b6f-44d2-b881-870632ffaa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for table: acquisition\n",
      "Extraction process successful for table: company\n",
      "Extraction process successful for table: funding_rounds\n",
      "Extraction process successful for table: funds\n",
      "Extraction process successful for table: investments\n",
      "Extraction process successful for table: ipos\n",
      "Extraction process successful for table: people\n",
      "Extraction process successful for table: relationships\n",
      "Extraction process successful for table: milestones\n"
     ]
    }
   ],
   "source": [
    "stg_acquisition = extract_staging(spark=spark, table_name=\"acquisition\")\n",
    "stg_company = extract_staging(spark=spark, table_name=\"company\")\n",
    "stg_funding_rounds = extract_staging(spark=spark, table_name=\"funding_rounds\")\n",
    "stg_funds = extract_staging(spark=spark, table_name=\"funds\")\n",
    "stg_investments = extract_staging(spark=spark, table_name=\"investments\")\n",
    "stg_ipos = extract_staging(spark=spark, table_name=\"ipos\")\n",
    "stg_people = extract_staging(spark=spark, table_name=\"people\")\n",
    "stg_relationships = extract_staging(spark=spark, table_name=\"relationships\")\n",
    "stg_milestones = extract_staging(spark=spark, table_name=\"milestones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2352d46-417b-41ad-8104-9c18ab56174f",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78b58cb1-ff9f-4efe-9e74-4045baf200aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(col_name: str):\n",
    "    \"\"\"\n",
    "    Cleans address values in a DataFrame column through the following steps:\n",
    "    1. Removing special characters '#' or '.' at the beginning of the string.\n",
    "    2. Converting the entire string to lowercase for standardization.\n",
    "    3. Identifying and replacing potentially invalid values with NULL.\n",
    "       A value is considered invalid if it consists solely of symbols and/or numbers,\n",
    "       or if its length after trimming leading and trailing spaces is less than or equal to 2 characters.\n",
    "\n",
    "    Parameters:\n",
    "        col_name (str): The name of the column containing the address values to clean.\n",
    "\n",
    "    Returns:\n",
    "        Column: A PySpark Column containing the cleaned address values.\n",
    "                Invalid values will be replaced with NULL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert to lowercase and remove '#' or '.' characters at the start of the string.\n",
    "    # Example: '#Main St' becomes 'main st', '.Apartment 1A' becomes 'apartment 1a'\n",
    "    cleaned = F.regexp_replace(F.lower(F.col(col_name)), r\"^[#.]+\", \"\")\n",
    "\n",
    "    # Step 2: Define conditions to identify invalid values.\n",
    "\n",
    "    # Condition 1: Check if the value (after step 1) consists solely of non-word characters\n",
    "    #              (symbols, spaces, punctuation), digits (numbers), or underscores.\n",
    "    #              Examples of values considered invalid: '??', '.323' (after removal becomes '323'), '------', ' !? '\n",
    "    is_only_symbols = cleaned.rlike(r\"^[\\W\\d_]+$\")\n",
    "\n",
    "    # Condition 2: Check if the length of the value (after step 1 and trimming) is too short.\n",
    "    #              Values with a length of 2 characters or less after trimming are considered invalid.\n",
    "    #              Examples of values considered invalid: 'a', ' b ', ''\n",
    "    is_too_short = F.length(F.trim(cleaned)) <= 2\n",
    "\n",
    "    # Step 3: Apply the cleaning logic.\n",
    "    # If a value meets either of the invalid conditions (only symbols or too short),\n",
    "    # then replace it with NULL. Otherwise, return the cleaned and trimmed value.\n",
    "    cleaned_data = F.when(\n",
    "        is_only_symbols | is_too_short,\n",
    "        F.lit(None)  # Replace invalid values with NULL\n",
    "    ).otherwise(\n",
    "        F.trim(cleaned)  # Keep and trim valid values\n",
    "    )\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fc1afa8-0ebd-4e70-ae0f-9a07e49b6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_usd(currency_col, amount_col):\n",
    "\n",
    "    exchange_rate = F.round(\n",
    "        F.when(F.col(currency_col) == \"USD\", F.col(amount_col))\n",
    "         .when(F.col(currency_col) == \"CAD\", F.col(amount_col) * 0.72)\n",
    "         .when(F.col(currency_col) == \"EUR\", F.col(amount_col) * 1.14)\n",
    "         .when(F.col(currency_col) == \"SEK\", F.col(amount_col) * 0.10)\n",
    "         .when(F.col(currency_col) == \"AUD\", F.col(amount_col) * 0.64)\n",
    "         .when(F.col(currency_col) == \"JPY\", F.col(amount_col) * 0.007)\n",
    "         .when(F.col(currency_col) == \"GBP\", F.col(amount_col) * 1.33)\n",
    "         .when(F.col(currency_col) == \"NIS\", F.col(amount_col) * 0.28)\n",
    "         .otherwise(F.col(amount_col)),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    return exchange_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd128ef7-f1f7-41af-b75f-8d46c4816c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_company(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw company data into a cleaned and standardized dimension table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing company data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed company dimension table\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Add a new column 'entity_type' to identify if the record is a 'company' or a 'fund'\n",
    "        # based on the 'object_id' prefix.\n",
    "        df = df.withColumn(\n",
    "            \"entity_type\",\n",
    "            F.when(F.col(\"object_id\").startswith(\"c:\"), \"company\")\n",
    "             .when(F.col(\"object_id\").startswith(\"f:\"), \"fund\")\n",
    "             .otherwise(None)\n",
    "        )\n",
    "\n",
    "        # Create cleaned versions of address columns by applying the 'clean_address' function.\n",
    "        df = df.withColumn(\"address1_cleaned\", clean_address(col_name=\"address1\")) \\\n",
    "               .withColumn(\"address2_cleaned\", clean_address(col_name=\"address2\"))\n",
    "\n",
    "        # Create the 'full_address' column by concatenating 'address1_cleaned' and 'address2_cleaned'.\n",
    "        # It handles cases where one or both address columns are null or empty.\n",
    "        df = df.withColumn(\n",
    "            \"full_address\",\n",
    "            F.when(\n",
    "                (F.col(\"address1_cleaned\").isNull()) & (F.col(\"address2_cleaned\").isNull()),\n",
    "                F.lit(None)\n",
    "            ).when(\n",
    "                (F.col(\"address1_cleaned\").isNull()) | (F.col(\"address1_cleaned\") == \"\"),\n",
    "                F.col(\"address2_cleaned\")\n",
    "            ).when(\n",
    "                (F.col(\"address2_cleaned\").isNull()) | (F.col(\"address2_cleaned\") == \"\"),\n",
    "                F.col(\"address1_cleaned\")\n",
    "            ).otherwise(\n",
    "                F.concat_ws(\", \", F.col(\"address1_cleaned\"), F.col(\"address2_cleaned\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Standardize the values for 'region', 'city', and 'country_code' by:\n",
    "        # 1. Trimming leading/trailing whitespace.\n",
    "        # 2. Converting 'region' and 'city' to lowercase.\n",
    "        # 3. Converting 'country_code' to uppercase.\n",
    "        region_cleaned = F.trim(F.lower(F.col(\"region\")))\n",
    "        city_cleaned = F.trim(F.lower(F.col(\"city\")))\n",
    "        country_code_cleaned = F.trim(F.upper(F.col(\"country_code\")))\n",
    "\n",
    "        # Update the 'region', 'city', and 'country_code' columns with the cleaned values,\n",
    "        # setting them to null if the cleaned value is null or empty.\n",
    "        df = df.withColumn(\n",
    "            \"region\",\n",
    "            F.when(\n",
    "                (region_cleaned.isNull()) | (region_cleaned == \"\"), F.lit(None)\n",
    "            ).otherwise(region_cleaned)\n",
    "        ).withColumn(\n",
    "            \"city\",\n",
    "            F.when(\n",
    "                (city_cleaned.isNull()) | (city_cleaned == \"\"), F.lit(None)\n",
    "            ).otherwise(city_cleaned)\n",
    "        ).withColumn(\n",
    "            \"country_code\",\n",
    "            F.when(\n",
    "                (country_code_cleaned.isNull()) | (country_code_cleaned == \"\"), F.lit(None)\n",
    "            ).otherwise(country_code_cleaned)\n",
    "        )\n",
    "\n",
    "        # Select the necessary columns for the dimension table and rename 'object_id' to 'nk_company_id'\n",
    "        # as the natural key.\n",
    "        dim_company = df.select(\n",
    "            F.col(\"object_id\").alias(\"nk_company_id\"),\n",
    "            F.col(\"entity_type\"),\n",
    "            F.col(\"full_address\"),\n",
    "            F.col(\"region\"),\n",
    "            F.col(\"city\"),\n",
    "            F.col(\"country_code\")\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: dim_company\")\n",
    "\n",
    "        # Log a success message with details about the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_company\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return dim_company\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"dim_company\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09ee0284-a089-4834-bf27-36d2e656c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: dim_company\n"
     ]
    }
   ],
   "source": [
    "dim_company = transform_company(spark, df=stg_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfdae603-8249-4f23-abc1-498b2342c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(col_name: str):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        col_name (str): The name of the column containing the address values to clean.\n",
    "\n",
    "    Returns:\n",
    "        Column: A PySpark Column containing the cleaned address values.\n",
    "                Invalid values will be replaced with NULL.\n",
    "    \"\"\"\n",
    "    # Clean leading non-alphabetic characters like #. or digits\n",
    "    cleaned = F.regexp_replace(F.lower(F.col(col_name)), r\"^[^a-z]+\", \"\")\n",
    "    \n",
    "    # Remove any remaining weird characters not allowed (keep only a-z, space, hyphen)\n",
    "    cleaned = F.regexp_replace(cleaned, r\"[^a-z\\s\\-]\", \"\")\n",
    "    \n",
    "    # Condition 1: string is only symbols/numbers after cleaning\n",
    "    is_only_symbols = cleaned.rlike(r\"^[^a-z]+$\")\n",
    "    \n",
    "    # Condition 2: too short\n",
    "    is_too_short = F.length(F.trim(cleaned)) <= 2\n",
    "    \n",
    "    # Condition 3: starts with a number or special char (extra safe)\n",
    "    starts_invalid = F.col(col_name).rlike(r\"^\\s*[\\d\\W_]+\")\n",
    "    \n",
    "    # Final decision\n",
    "    cleaned_data = F.when(\n",
    "        is_only_symbols | is_too_short | starts_invalid,\n",
    "        F.lit(None)\n",
    "    ).otherwise(\n",
    "        F.trim(cleaned)\n",
    "    )\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09606bd3-f381-4eb6-b6f7-7289b64451c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform_people(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw people data into a cleaned and standardized dimension table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing people data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed people dimension table\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Add cleaned versions of first and last name, using the clean_name function.\n",
    "        df = df.withColumn(\"firstname_cleaned\", clean_name(col_name=\"first_name\")) \\\n",
    "               .withColumn(\"lastname_cleaned\", clean_name(col_name=\"last_name\"))\n",
    "\n",
    "        # Create the 'full_name' column by concatenating cleaned first and last names.\n",
    "        # Handles cases where one or both cleaned names are null or empty.\n",
    "        df = df.withColumn(\n",
    "            \"full_name\",\n",
    "            F.when(\n",
    "                (F.col(\"firstname_cleaned\").isNull()) & (F.col(\"lastname_cleaned\").isNull()),\n",
    "                F.lit(None)\n",
    "            ).when(\n",
    "                (F.col(\"firstname_cleaned\").isNull()) | (F.col(\"firstname_cleaned\") == \"\"),\n",
    "                F.col(\"lastname_cleaned\")\n",
    "            ).when(\n",
    "                (F.col(\"lastname_cleaned\").isNull()) | (F.col(\"lastname_cleaned\") == \"\"),\n",
    "                F.col(\"firstname_cleaned\")\n",
    "            ).otherwise(\n",
    "                F.concat_ws(\" \", F.col(\"firstname_cleaned\"), F.col(\"lastname_cleaned\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Standardize affiliation name by trimming whitespace and converting to lowercase.\n",
    "        df = df.withColumn(\n",
    "            \"affiliation_name\",\n",
    "            F.trim(F.lower(F.col(\"affiliation_name\")))\n",
    "        )\n",
    "\n",
    "        # Select the columns for the dimension table.\n",
    "        # Rename 'object_id' to 'nk_people_id'.\n",
    "        dim_people = df.select(\n",
    "            F.col(\"object_id\").alias(\"nk_people_id\"),\n",
    "            F.col(\"full_name\"),\n",
    "            F.col(\"affiliation_name\")\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: dim_people\")\n",
    "\n",
    "        # Log a success message with details about the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_people\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return dim_people\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"dim_people\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b9fb5b7-cf67-40be-97d7-3c73974b719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: dim_people\n"
     ]
    }
   ],
   "source": [
    "dim_people = transform_people(spark, df=stg_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12cca8cc-5462-4622-94c0-867ad8d40b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_warehouse(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS = dwh_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DWH_DB_USER,\n",
    "        \"password\": DWH_DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DWH_DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b394647a-019c-471f-a770-64202466e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_funds(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw funds data into a cleaned and standardized dimension table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing fund data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed funds dimension table\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extract the dim_date dimension table.\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "\n",
    "        # Standardize 'name' and 'source_description' by trimming whitespace and converting to lowercase.\n",
    "        df = df.withColumn(\"name\", F.trim(F.lower(F.col(\"name\")))) \\\n",
    "               .withColumn(\"source_description\", F.trim(F.lower(F.col(\"source_description\"))))\n",
    "\n",
    "        # Convert raised amount to USD using the to_usd function.\n",
    "        df = df.withColumn(\"raised_amount_usd\", to_usd(currency_col=\"raised_currency_code\", amount_col=\"raised_amount\"))\n",
    "\n",
    "        # Add a foreign key 'funded_date_id' by formatting 'funded_at' to match the 'date_id' in dim_date.\n",
    "        df = df.withColumn(\n",
    "            \"funded_date_id\",\n",
    "            F.date_format(df.funded_at, \"yyyyMMdd\").cast(\"integer\")\n",
    "        )\n",
    "\n",
    "        # Join with dim_date to get date information based on 'funded_date_id'.\n",
    "        df = df.join(\n",
    "            dim_date,\n",
    "            df.funded_date_id == dim_date.date_id,\n",
    "            \"left\"\n",
    "        )\n",
    "\n",
    "        # Remove empty strings from 'fund_description' and set them to NULL.\n",
    "        df = df.withColumn(\n",
    "            \"source_description\",\n",
    "            F.when(F.trim(df.source_description) == \"\", None)\n",
    "              .otherwise(df.source_description)\n",
    "        )\n",
    "\n",
    "        # Select the columns for the dimension table and rename for clarity.\n",
    "        dim_fund = df.select(\n",
    "            F.col(\"object_id\").alias(\"nk_fund_id\"),\n",
    "            F.col(\"name\").alias(\"fund_name\"),\n",
    "            F.col(\"raised_amount_usd\"),\n",
    "            F.col(\"funded_date_id\").alias(\"funded_at\"),\n",
    "            F.col(\"source_description\").alias(\"fund_description\")\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: dim_fund\")\n",
    "\n",
    "        # Log a success message with details about the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"dim_fund\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return dim_fund\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"dim_fund\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e634df1f-f970-4716-8650-180dba9e253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: dim_fund\n"
     ]
    }
   ],
   "source": [
    "dim_funds = transform_funds(spark, df=stg_funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f726a744-baf7-4407-9007-3baa741b2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_relationship(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw relationships data into a cleaned and standardized dimension table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing relationships data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed bridge people company dimension table\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extract dimension tables needed for joins.\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "        dim_company = extract_warehouse(spark, table_name=\"dim_company\")\n",
    "        dim_people = extract_warehouse(spark, table_name=\"dim_people\")\n",
    "\n",
    "        # Join with dim_company to get the company's surrogate key (sk_company_id).\n",
    "        df = df.join(\n",
    "            dim_company.select(\"sk_company_id\", \"nk_company_id\"),\n",
    "            df.relationship_object_id == dim_company.nk_company_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Join with dim_people to get the person's surrogate key (sk_people_id).\n",
    "        df = df.join(\n",
    "            dim_people.select(\"sk_people_id\", \"nk_people_id\"),\n",
    "            df.person_object_id == dim_people.nk_people_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Convert start and end dates to integer format (yyyyMMdd) for consistency.\n",
    "        df = df.withColumn(\"relationship_start_at\", F.date_format(\"start_at\", \"yyyyMMdd\").cast(\"integer\")) \\\n",
    "               .withColumn(\"relationship_end_at\", F.date_format(\"end_at\", \"yyyyMMdd\").cast(\"integer\"))\n",
    "\n",
    "        # Alias the dim_date table for joining on start and end dates.\n",
    "        dim_date_start = dim_date.alias(\"start_date\")\n",
    "        dim_date_end = dim_date.alias(\"end_date\")\n",
    "\n",
    "        # Join with dim_date to get date information for start and end dates.\n",
    "        df = df.join(\n",
    "            dim_date_start,\n",
    "            df.relationship_start_at == F.col(\"start_date.date_id\"),\n",
    "            \"inner\"\n",
    "        ).join(\n",
    "            dim_date_end,\n",
    "            df.relationship_end_at == F.col(\"end_date.date_id\"),\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Clean the 'title' column by trimming whitespace, converting to lowercase, and setting '.' to NULL.\n",
    "        df = df.withColumn(\n",
    "            \"title\",\n",
    "            F.when(F.col(\"title\") == \".\", F.lit(None))\n",
    "             .otherwise(F.trim(F.lower(F.col(\"title\"))))\n",
    "        )\n",
    "\n",
    "        # Select the columns for the bridge table.\n",
    "        bridge_company_people = df.select(\n",
    "            F.col(\"sk_company_id\"),\n",
    "            F.col(\"sk_people_id\"),\n",
    "            F.col(\"title\"),\n",
    "            F.col(\"is_past\"),\n",
    "            F.col(\"relationship_start_at\"),\n",
    "            F.col(\"relationship_end_at\")\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: bridge_company_people\")\n",
    "\n",
    "        # Log a success message for the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"bridge_company_people\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return bridge_company_people\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"bridge_company_people\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09cf0d40-e4bf-4ba3-a647-44ba64e98734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: bridge_company_people\n"
     ]
    }
   ],
   "source": [
    "bridge_company_people = transform_relationship(spark, df=stg_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bc58a11-8a23-4794-a0e5-0af17db96a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_investments(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw investments and funding rounds data into a cleaned and standardized fact table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing investments data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed investments fact table\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extract dimension tables needed for joins.\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "        dim_company = extract_warehouse(spark, table_name=\"dim_company\")\n",
    "        dim_funds = extract_warehouse(spark, table_name=\"dim_funds\")\n",
    "        stg_funding_rounds = extract_staging(spark, table_name=\"funding_rounds\")\n",
    "\n",
    "        # Join with dim_company to get the company's surrogate key (sk_company_id).\n",
    "        df = df.join(\n",
    "            dim_company.select(\"sk_company_id\", \"nk_company_id\"),\n",
    "            df.funded_object_id == dim_company.nk_company_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Join with dim_fund to get the fund's surrogate key (sk_fund_id).\n",
    "        df = df.join(\n",
    "            dim_funds.select(\"sk_fund_id\", \"nk_fund_id\"),\n",
    "            df.investor_object_id == dim_funds.nk_fund_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Prepare staging funding rounds data.\n",
    "        stg_funding_rounds = stg_funding_rounds.withColumn(\n",
    "            \"funded_at\",\n",
    "            F.date_format(\"funded_at\", \"yyyyMMdd\").cast(\"integer\")\n",
    "        )\n",
    "\n",
    "        # Join with dim_date to get date_id.\n",
    "        stg_funding_rounds = stg_funding_rounds.join(\n",
    "            dim_date.select(\"date_id\"),\n",
    "            stg_funding_rounds.funded_at == dim_date.date_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Join with the funding rounds staging table to get additional information.\n",
    "        df = df.join(\n",
    "            stg_funding_rounds.select(\n",
    "                \"funding_round_id\", \"funding_round_type\", \"participants\",\n",
    "                \"raised_amount_usd\", \"raised_currency_code\",\n",
    "                \"pre_money_valuation_usd\", \"post_money_valuation_usd\",\n",
    "                \"funded_at\"\n",
    "            ),\n",
    "            on=\"funding_round_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Select the columns for the fact table and rename for clarity.\n",
    "        fct_investments = df.select(\n",
    "            F.col(\"investment_id\").alias(\"dd_investment_id\"),\n",
    "            F.col(\"sk_company_id\"),\n",
    "            F.col(\"sk_fund_id\"),\n",
    "            F.col(\"funded_at\"),\n",
    "            F.col(\"funding_round_type\"),\n",
    "            F.col(\"participants\").alias(\"num_of_participants\"),\n",
    "            F.col(\"raised_amount_usd\"),\n",
    "            F.col(\"pre_money_valuation_usd\"),\n",
    "            F.col(\"post_money_valuation_usd\"),\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: fct_investments\")\n",
    "\n",
    "        # Log a success message for the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fct_investments\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return fct_investments\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"fct_investments\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38f67dbf-bd72-4554-9301-7605291b3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process successful for table: funding_rounds\n",
      "Transformation process successful for table: fct_investments\n"
     ]
    }
   ],
   "source": [
    "fct_investments = transform_investments(spark, df=stg_investments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f747d68c-d6d9-4466-82c1-9fe0a6d7fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ipos(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw IPOs data into a cleaned and standardized fact table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing ipos data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed ipos fact table\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extract dimension tables needed for joins.\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "        dim_company = extract_warehouse(spark, table_name=\"dim_company\")\n",
    "\n",
    "        # Cast 'ipo_id' to integer.\n",
    "        df = df.withColumn(\"ipo_id\", F.col(\"ipo_id\").cast(\"integer\"))\n",
    "\n",
    "        # Join with dim_company to get the company's surrogate key (sk_company_id).\n",
    "        df = df.join(\n",
    "            dim_company.select(\"sk_company_id\", \"nk_company_id\"),\n",
    "            df.object_id == dim_company.nk_company_id,\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Add a foreign key 'public_date_id' by formatting 'public_at' to match 'date_id' in dim_date.\n",
    "        df = df.withColumn(\n",
    "            \"public_date_id\",\n",
    "            F.date_format(df.public_at, \"yyyyMMdd\").cast(\"integer\")\n",
    "        )\n",
    "\n",
    "        # Join with dim_date to get date information based on 'public_date_id'.\n",
    "        df = df.join(\n",
    "            dim_date,\n",
    "            df.public_date_id == dim_date.date_id,\n",
    "            \"left\"\n",
    "        )\n",
    "\n",
    "        # Convert valuation and raised amounts to USD.\n",
    "        df = df.withColumn(\"valuation_amount_usd\", to_usd(currency_col=\"valuation_currency_code\", amount_col=\"valuation_amount\"))\n",
    "        df = df.withColumn(\"raised_amount_usd\", to_usd(currency_col=\"raised_currency_code\", amount_col=\"raised_amount\"))\n",
    "\n",
    "        # Clean and normalize the stock symbol.\n",
    "        cleaned_stock_symbol = F.trim(F.lower(F.col(\"stock_symbol\")))\n",
    "        invalid_symbol = cleaned_stock_symbol.rlike(r\"^[\\W\\d_]+$\")  # Identify invalid symbols\n",
    "        cleaned_data = F.when(invalid_symbol, F.lit(None)).otherwise(cleaned_stock_symbol) # Replace invalid with NULL\n",
    "        df = df.withColumn(\"stock_symbol\", cleaned_data)\n",
    "\n",
    "        # Remove unused whitespace and convert values to lowercase.\n",
    "        df = df.withColumn(\"source_description\", F.trim(F.lower(F.col(\"source_description\"))))\n",
    "\n",
    "        # Select the columns for the fact table and rename for clarity.\n",
    "        fct_ipos = df.select(\n",
    "            F.col(\"ipo_id\").alias(\"dd_ipo_id\"),\n",
    "            F.col(\"sk_company_id\"),\n",
    "            F.col(\"valuation_amount_usd\"),\n",
    "            F.col(\"raised_amount_usd\"),\n",
    "            F.col(\"public_date_id\").alias(\"public_at\"),\n",
    "            F.col(\"stock_symbol\"),\n",
    "            F.col(\"source_description\").alias(\"ipo_description\")\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: fct_ipos\")\n",
    "\n",
    "        # Log a success message for the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fct_ipos\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return fct_ipos\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"fct_ipos\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a89a5ad3-c7fa-4f06-b384-f103c4605687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: fct_ipos\n"
     ]
    }
   ],
   "source": [
    "fct_ipos = transform_ipos(spark, df=stg_ipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f51032c-90fb-4226-868f-7467be6965af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_acquisition(spark, df):\n",
    "    \"\"\"\n",
    "    Transform raw acquisition data into a cleaned and standardized fact table.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        df: Raw input DataFrame containing acquisition data from staging db\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and transformed acquisition fact table\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up current timestamp for logging\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extract dimension tables needed for joins.\n",
    "        dim_date = extract_warehouse(spark, table_name=\"dim_date\")\n",
    "        dim_company = extract_warehouse(spark, table_name=\"dim_company\")\n",
    "\n",
    "        # Set alias for acquiring and acquired companies from dim_company.\n",
    "        dim_company_acquiring = dim_company.alias(\"acq\")\n",
    "        dim_company_acquired = dim_company.alias(\"acd\")\n",
    "\n",
    "        # Join with dim_company to get surrogate keys for acquiring and acquired companies.\n",
    "        df = df.join(\n",
    "            dim_company_acquiring.select(\n",
    "                F.col(\"sk_company_id\").alias(\"sk_acquiring_company_id\"),\n",
    "                F.col(\"nk_company_id\").alias(\"nk_acquiring_company_id\")\n",
    "            ),\n",
    "            df.acquiring_object_id == F.col(\"nk_acquiring_company_id\"),\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        df = df.join(\n",
    "            dim_company_acquired.select(\n",
    "                F.col(\"sk_company_id\").alias(\"sk_acquired_company_id\"),\n",
    "                F.col(\"nk_company_id\").alias(\"nk_acquired_company_id\")\n",
    "            ),\n",
    "            df.acquired_object_id == F.col(\"nk_acquired_company_id\"),\n",
    "            \"inner\"\n",
    "        )\n",
    "\n",
    "        # Add a foreign key 'acquired_date_id' by formatting 'acquired_at' to match 'date_id' in dim_date.\n",
    "        df = df.withColumn(\n",
    "            \"acquired_date_id\",\n",
    "            F.date_format(df.acquired_at, \"yyyyMMdd\").cast(\"integer\")\n",
    "        )\n",
    "\n",
    "        # Join with dim_date to get date information based on 'acquired_date_id'.\n",
    "        df = df.join(\n",
    "            dim_date,\n",
    "            df.acquired_date_id == dim_date.date_id,\n",
    "            \"left\"\n",
    "        )\n",
    "\n",
    "        # Convert acquisition price to USD.\n",
    "        df = df.withColumn(\"price_amount_usd\", to_usd(currency_col=\"price_currency_code\", amount_col=\"price_amount\"))\n",
    "\n",
    "        # Clean and normalize term code.\n",
    "        cleaned_term_code = F.trim(F.lower(F.col(\"term_code\")))\n",
    "        df = df.withColumn(\"term_code\", F.when(cleaned_term_code == \"\", F.lit(None)).otherwise(cleaned_term_code))\n",
    "\n",
    "        # Clean the source description.\n",
    "        cleaned_description = F.trim(F.lower(F.col(\"source_description\")))\n",
    "        df = df.withColumn(\n",
    "            \"source_description\",\n",
    "            F.when(cleaned_description == \"\", F.lit(None))\n",
    "             .otherwise(cleaned_description)\n",
    "        )\n",
    "\n",
    "        # Select the columns for the fact table and rename for clarity.\n",
    "        fct_acquisition = df.select(\n",
    "            F.col(\"acquisition_id\").alias(\"dd_acquisition_id\"),\n",
    "            F.col(\"sk_acquiring_company_id\"),\n",
    "            F.col(\"sk_acquired_company_id\"),\n",
    "            F.col(\"price_amount_usd\"),\n",
    "            F.col(\"acquired_date_id\").alias(\"acquired_at\"),\n",
    "            F.col(\"term_code\"),\n",
    "            F.col(\"source_description\").alias(\"acquisition_description\")\n",
    "        )\n",
    "\n",
    "        print(\"Transformation process successful for table: fct_acquisition\")\n",
    "\n",
    "        # Log a success message for the ETL process.\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fct_acquisition\", current_timestamp)]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "        return fct_acquisition\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        # Log an error message with details about the failure.\n",
    "        print(e)\n",
    "        log_msg = spark.sparkContext \\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"fct_acquisition\", current_timestamp, str(e))]) \\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c518cf58-618d-42f0-891a-7c4be33ae240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation process successful for table: fct_acquisition\n"
     ]
    }
   ],
   "source": [
    "fct_acquisition = transform_acquisition(spark, df=stg_acquisition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca5639-1907-40d0-ab6c-99991046263a",
   "metadata": {},
   "source": [
    "## Load to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aaaffeb0-a6f3-42c3-9334-5fc5fd56de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_dwh(spark, df, table_name, source_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Establish connection to warehouse db\n",
    "        conn = dwh_engine_sqlalchemy()\n",
    "\n",
    "        with conn.begin() as connection:\n",
    "\n",
    "            # Truncate all tables in data warehouse\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "\n",
    "        print(f\"Success truncating table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error when truncating table: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "\n",
    "    finally:\n",
    "        conn.dispose()\n",
    "\n",
    "    # Load extarcted DataFrame to warehouse db\n",
    "    try:\n",
    "        \n",
    "        # Get warehouse db config\n",
    "        DWH_DB_URL, DWH_DB_USER, DWH_DB_PASS = dwh_engine()\n",
    "    \n",
    "        # Set config\n",
    "        properties = {\n",
    "            \"user\" : DWH_DB_USER,\n",
    "            \"password\" : DWH_DB_PASS,\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=DWH_DB_URL,\n",
    "                      table=table_name,\n",
    "                      mode=\"append\",\n",
    "                      properties=properties)\n",
    "\n",
    "        print(f\"Load process successful for table: {table_name}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        load_log_msg(spark=spark, log_msg=log_message) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Load process failed: {e}\")\n",
    "\n",
    "        log_message = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log_msg(spark=spark, log_msg=log_message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1c6ab97-5d82-4453-b3dc-5c98d8ade0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: dim_company\n",
      "Load process successful for table: dim_company\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=dim_company ,table_name=\"dim_company\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5752696-ca35-4c95-bbd4-3bbdf45ea452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: dim_people\n",
      "Load process successful for table: dim_people\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=dim_people, table_name=\"dim_people\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae51ca72-165a-4b40-9770-6b9cbba4fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: dim_funds\n",
      "Load process successful for table: dim_funds\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=dim_funds, table_name=\"dim_funds\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6988525c-ab95-445f-9983-d142166e37be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: bridge_company_people\n",
      "Load process successful for table: bridge_company_people\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=bridge_company_people, table_name=\"bridge_company_people\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40bb235e-0081-42df-9aca-b81e74c582d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: fct_investments\n",
      "Load process successful for table: fct_investments\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=fct_investments, table_name=\"fct_investments\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fef260c1-7546-401a-918a-99543e879568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: fct_ipos\n",
      "Load process successful for table: fct_ipos\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=fct_ipos, table_name=\"fct_ipos\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a8badae-3487-4646-992a-d0141ea58ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success truncating table: fct_acquisition\n",
      "Load process successful for table: fct_acquisition\n"
     ]
    }
   ],
   "source": [
    "load_to_dwh(spark, df=fct_acquisition, table_name=\"fct_acquisition\", source_name=\"staging_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8716b-c219-4ea9-9229-c07ed41eaa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
